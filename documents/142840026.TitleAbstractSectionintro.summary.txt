

This section discusses the use of abstract interpretation to build trust and safety in artificial intelligence with deep neural networks. It explains that standard training and testing of DNNs does not take into account desirable safety properties such as robustness, fairness, and monotonicity. To address these limitations, there is growing work on checking the safety of DNN models and interpreting their behavior on an infinite set of unseen inputs using formal certification. The certification problem consists of two main components: a trained DNN and a property specification in the form of a tuple containing symbolic formulas. The certification can be seen as an instance of program verification, making it undecidable. State-of-the-art certifiers are formulated using the framework of abstract interpretation. The certifier first computes an abstract element that includes the input region, then symbolically propagates it through the different layers of the network. At each layer, the analyzer computes an abstract element overapproximating the exact layer output corresponding to the input region.