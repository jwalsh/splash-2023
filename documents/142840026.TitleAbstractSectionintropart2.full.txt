Title-Abstract. Section intro
 approximates the effect of the operations (e.g.,
ReLU, affine) applied at the layer. Propagation through all the layers yields an
abstract element g(α(ϕ)) ⊇ f (ϕ) at the output layer. Next, the certifier checks
if g(α(ϕ)) ⊆ ψ holds for the bigger region g(α(ϕ)). If the answer is yes, then
f (ϕ) ⊆ ψ also holds for the smaller region f (ϕ). Because of the overapproxima-

2


Fig. 2: Development pipeline for building fast, accurate, and trustworthy DNNs.
Certification is used for testing model trustworthiness (green diamond).

tion, it can be the case that g(α(ϕ)) ⊆ ψ does not hold while f (ϕ) ⊆ ψ holds.
To reduce the amount of overapproximation, refinements [62, 53, 41, 47, 63, 68,
71] can be applied.

To obtain an effective certifier, it is essential to design an abstract domain
and corresponding abstract transformers such that g(α(ϕ)) is as close as pos-
sible to the true output f (ϕ) while g can also be computed in a reasonable
amount of time for practical networks. The classical domains, such as Poly-
hedra [16, 55] and Octagons [37, 54], used for analyzing programs are not well
suited for DNN certification. This is because the DNNs have a different struc-
ture compared to traditional programs. For example, DNNs have a large number
of non-linear assignments but typically do not have infinite loops. For efficient
certification, new abstract domains and transformers tailored for DNN certifi-
cation have been developed. Examples include DeepPoly [52], DeepZ [51], Star
sets [58], and DeepJ [33]. These custom solutions can scale to realistic DNNs with
upto a million neurons [39], or more than 100 layers [68], certifying diverse safety
properties in different real-world applications including autonomous driving [72],
job-scheduling [68], data center management [12], and financial modeling [32].
Incremental certification. By leveraging formal certification to check DNN
safety and trust, the development pipeline shown in Figure 2 can be employed [61]
to obtain fast, accurate, and trustworthy DNNs. First, a DNN is trained to maxi-
mize its test accuracy. Next, a domain expert designs a set of safety specifications
(e.g., robustness, fairness) defining the expected network behavior in different
real-world scenarios. If the model satisfies the desired specifications, then the
DNN is considered fit for deployment. Otherwise, it is iteratively repaired (e.g.,
by fine-tuning [1] or LP-solving [56]) till we obtain a fast, accurate, and trust-
worthy DNN. We note that repair is preferred over retraining as it is cheaper.
However, if a repair is not possible, then the DNN is retrained from scratch.
After deployment, the DNN is monitored to check for distribution shifts, gener-

3


ating inputs not covered by the specifications. If a distribution shift is detected,
then new specifications are designed, and the model is repaired or retrained.

Domain experts usually design a large number of local properties (around
10-100K). Therefore, the certifier needs to be run several thousand times on the
same DNN. Further, as shown in Figure 2, the model repair is applied, before
or after deployment, in case the DNN does not satisfy the desired specifications.
The certifier is needed again to check the safety of the repaired model. Existing
certifiers do not scale in such a deployment setting: they can precisely certify
individual specifications in a few seconds or minutes, however, the certification
of a large and diverse set of specifications on a single DNN can take multiple
days to years or the certifier can run out of memory. Given multiple DNNs are
generated due to repair or retraining, it makes using existing certifiers for safe
and trustworthy development infeasible. The inefficiency is because the certifier
needs to be run from scratch for every new pair of specifications and DNNs.
A straightforward approach to overcome this limitation is to run the certifier
on several machines. However, such an approach is not sustainable due to its
huge environmental cost
[67, 8]. Further, in many cases, large computational
resources are not available. For example, to preserve privacy, reduce latency, and
increase battery lifetime, DNNs are increasingly employed on edge devices with
limited computational power [64, 14]. Therefore, for sustainable, democratic, and
trustworthy DNN development, it is essential to develop new general approaches
for incremental certification to improve the certifier scalability, when certifying
multiple specifications and networks.

The main challenge in developing incremental certifiers is determining in-
formation that (i) can be reused across multiple specifications and DNNs to
improve scalability, and (ii) is efficient to compute and store. Recent works [19,
61] have developed general mechanisms to enable incremental certification by
reusing proofs across multiple specifications and DNNs. These methods can be
plugged into state-of-the-art certifiers based on abstract interpretation [52, 51]
to improve their scalability inside the development pipeline of Figure 2.
[19] in-
troduced the concept of proof sharing across multiple specifications on the same
DNN. Proof sharing is based on the key insight that it is possible to construct a
small number of abstract elements as proof templates at an intermediate DNN
layer, that capture the intermediate proofs of a large number of specifications.
To certify a new specification, we run the certifier partially till the layer at which
the templates are available. If the intermediate proof is subsumed by an exist-
ing template, then the specification is proved without running the certifier till
the end, saving time and memory. The work of [61] introduced the concept of
proof transfer across similar networks obtained after incremental changes to an
original network (e.g., after fine-tuning [1]). The key insight behind this con-
cept is that it is possible to efficiently transfer the proof templates generated
on the original network to multiple similar networks, such that the transformed
templates capture the proofs of a large number of specifications on similar net-
works. The transferred templates can improve certifier precision and scalability
when certifying multiple specifications on similar networks.
[60] considers incre-

4


Fig. 3: Certified training involves computing the point z ∈ g(α(ϕ)) where the ro-
bust loss if maximum. The resulting loss is backpropagated through the certifier
code to update the model parameters.

mental certification for certifiers combining abstract interpretation with branch
and bound (BaB) [11] and uses the trace of BaB as proof templates to improve
certification speed across multiple similar DNNs.

3 Certification for Training Safe DNNs

DNNs trained to only maximize accuracy with standard training [28] are often
unsafe [36]. Next, we describe how certifiers can be leveraged during training to
obtain safe DNNs. While the description here applies to different safety proper-
ties, we focus on robustness as it is the most common property for safe training
considered in the literatureust training involves defining a robust loss func-
tion LR for each point x ∈ ϕ with the property that LR at x is ≤ 0 iff in the
DNN output z = f (x), the score zc for the correct class c is higher than all other
classes zi, i.e., zc > zi. The DNN is robust iff LR ≤ 0 for all x ∈ ϕ. The DNN
parameters can be updated during training to minimize the maximum value
of LR. This min-max formulation makes robust training a harder optimization
problem than standard training. Computing the worst-case robust loss exactly
requires computing f (ϕ) which is infeasible. Therefore an approximation of LR is
computed in practice. Adversarial training methods [36] compute a lower bound
on the worst-case robust loss by heuristically computing a point x ∈ ϕ at which
the robust loss is high. x is then added to the training dataset. On the other
hand, certified training [72, 38, 69, 74,