

This paper discusses the use of abstract interpretation to build trust and safety in artificial intelligence with deep neural networks (DNNs). It explains that standard training and testing of DNNs does not take into account desirable safety properties such as robustness, fairness, and monotonicity. To address these limitations, there is growing work on checking the safety of DNN models and interpreting their behavior on an infinite set of unseen inputs using formal certification. The certification problem consists of two main components: a trained DNN and a property specification in the form of a tuple containing symbolic formulas. The certification can be seen as an instance of program verification, making it undecidable. State-of-the-art certifiers are formulated using the framework of abstract interpretation.

The paper then discusses the development of certifiers for efficient and trustworthy DNNs. It explains how domain experts design a set of safety specifications to define the expected network behavior in different real-world scenarios. It also explains the inefficiency of existing certifiers when certifying a large and diverse set of specifications on a single DNN. To overcome this limitation, recent works have developed general mechanisms to enable incremental certification by reusing proofs across multiple specifications and DNNs. This section also discusses certified training, which involves defining a robust loss function LR for each point x in the DNN output z = f (x). The DNN parameters can be updated during training to minimize the maximum value of LR. Adversarial training methods compute a lower bound on the worst-case robust loss by heuristically computing a point x at which the robust loss is high. Certified training, on the other hand, involves computing the point z in g(α(ϕ)) where the robust loss is maximum and backpropagating the resulting loss to update the model parameters.

The paper then discusses the challenge of interpreting DNN proofs, and introduces the work of [7] which develops a method for interpreting robustness proofs computed by DNN certifiers. Finally, the paper compares proof interpretations for DNNs trained with standard and robust training methods on the MNIST and CIFAR10 datasets, and shows that DNNs can satisfy safety properties but their behavior can still be untrustworthy.
