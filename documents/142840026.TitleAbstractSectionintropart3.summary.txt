

This section discusses the use of abstract interpretation-based DNN certifiers to compute an upper bound on the worst-case robust loss. It explains how certified training is easier to certify than adversarial and standard training, and how it can achieve higher robustness guarantees. It also explains how combining the robust loss with standard accuracy loss during training can improve model accuracy. The section then discusses the challenge of interpreting DNN proofs, and introduces the work of [7] which develops a method for interpreting robustness proofs computed by DNN certifiers. Finally, the section compares proof interpretations for DNNs trained with standard and robust training methods on the MNIST and CIFAR10 datasets, and shows that DNNs can satisfy safety properties but their behavior can still be untrustworthy.